<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="icon" href="./images/icon.png">
<link rel="stylesheet" href="./css/main.css" type="text/css" />
<link rel="stylesheet" href="font-awesome/css/font-awesome.min.css">
<!--- <title>Supervised Learning Problem</title> --->
<title>Federico Ang</title>
<!-- MathJax -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async>
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
	  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
<!-- End MathJax -->
</head>
<body>
<div id="main-container">
<div id="header-container">
<div id="header">
<div id="header-icon-text-container">
<div id="header-icon-container" >
<a href="index.html"><img src="./images/fed.jpg" alt="Fed" style="width: 100%; height: 100%; position: center; padding:0px; margin: 0px;"></a>
</div>
<div id="header-text-container">
<a href="index.html">Federico Ang</a>
</div>
</div>
<div id="main">
<button class="openbtn" onclick="openNav()">☰</button>
</div>
</div>
</div>
<div id="layout">
<div id="layout-menu-container">
<div id="layout-menu">
<div class="menu-item"><a href="javascript:void(0)" class="closebtn" onclick="closeNav()">×</a></div>
<div class="menu-category">Federico Ang</div>
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-item"><a href="blog.html">Blog</a></div>
<div class="menu-category">Problem Bank</div>
<div class="menu-item"><a href="mlpb.html" class="current">Machine&nbsp;Learning</a></div>
<div class="menu-item"><a href="pspb.html">Probability&nbsp;and&nbsp;Statistics</a></div>
<div class="menu-category">External Links</div>
<div class="menu-item"><a href="https://www.youtube.com" target="blank">YouTube&nbsp;Channel&nbsp;(maybe!)</a></div>
</div> <!-- <div id="layout-menu"> -->
</div> <!-- <div id="layout-menu-container"> -->
<div id="layout-content-container">
<div id="layout-content">
<div id="toptitle">
<h1>Supervised Learning Problem</h1>
<div id="subtitle">From Stanford's CS 229</div>
</div>
<p><a href="mlpb.html" target=&ldquo;blank&rdquo;>Back to top</a></p>
<h2>Naive Bayes</h2>
<p>In this problem, we look at maximum likelihood parameter estimation using the naive Bayes assumption.
Here, the input features \(x_j\), \(j=1,\ldots,n\) to our model are discrete, binary-valued variables,
so \(x_j\in\{0,1\}\).  We call \(x=[x_1&nbsp;x_2&nbsp;\cdots&nbsp;x_n]^T\) to be the input vector.  For each training example,
out output targets are a single binary-value \(y\in\{0,1\}\).  Our model is then parameterized by
\(\phi_{j|y=0}=p(x_j=1|y=0)\), \(\phi_{j|y=1}=p(x_j=1|y=1)\), and \(\phi_y=p(y=1)\).  We model the joint distribution
of \((x,y)\) according to</p>
<p style="text-align:center">
\[
  \begin{eqnarray*}
	  p(y) &amp;=&amp; (\phi_y)^y(1-\phi_y)^{1-y} \\
	  p(x|y=0) &amp;=&amp; \prod_{j=1}^np(x_j|y=0) \\
		   &amp;=&amp; \prod_{j=1}^n(\phi_{j|y=0})^{x_j}(1-\phi_{j|y=0})^{1-x_j} \\
	  p(x|y=1) &amp;=&amp; \prod_{j=1}^np(x_j|y=1) \\
		   &amp;=&amp; \prod_{j=1}^n(\phi_{j|y=1})^{x_j}(1-\phi_{j|y=1})^{1-x_j} \\
	\end{eqnarray*}
  \]
</p><ul>
<li><p>Find the joint likelihood function \(\ell(\varphi)=\log\prod_{i=1}^mp(x^{(i)},y^{(i)};\varphi)\) in terms of the
model parameters given above.  Here, \(\varphi\) represents the entire set of parameters \(\{\phi_y,\phi_{j|y=0},\phi_{j|y=1},j=1,\ldots,n\}\).</p>
</li>
</ul>
<p><ul style="list-style-type: none";><li><p><strong>Ans.</strong></p></li></ul></p>
<p style="text-align:center">
\[
        \begin{eqnarray*}
		\ell(\varphi)&amp;=&amp;\log{\prod_{i=1}^m{p(x^{(i)},y^{(i)};\varphi)}}\\
			     &amp;=&amp;\log{\prod_{i=1}^m{p(x^{(i)}|y^{(i)};\varphi)p(y^{(i)};\varphi)}}\\
			     &amp;=&amp;\sum_{i=1}^m{\left[\log{p(y^{(i)};\phi_y)}+\log{p(x^{(i)}|y^{(i)};\phi_{j|y^{(i)}})}\right]}\\
			     &amp;=&amp;\sum_{i=1}^m{\left[\log{(\phi_y)^{y^{(i)}}(1-\phi_y)^{1-y^{(i)}}}+\log{\prod_{j=1}^n{(\phi_{j|y^{(i)}})^{x_j^{(i)}}(1-\phi_{j|y^{(i)}})^{1-x_j^{(i)}}}}\right]}\\
	      \end{eqnarray*}
        \]
</p><p><ul style="list-style-type: none";><li><p>
Finally,</p></li></ul></p>
<p style="text-align:center">
\[
	      \boxed{\ell(\varphi)=\sum_{i=1}^m{\left[y^{(i)}\log{\phi_y}+(1-y^{(i)})\log{(1-\phi_y)}+\sum_{j=1}^n{x_j^{(i)}\log{\phi_{j|y^{(i)}}}+(1-x_j^{(i)})\log{(1-\phi_{j|y^{(i)}})}}\right]}}
	      \]
</p><ul>
<li><p>Show that the parameters which maximize the likelihood function are the following:</p>
</li>
</ul>
<p style="text-align:center">
\[
      \begin{eqnarray*}
	      \phi_{j|y=0} &amp;=&amp; \frac{\sum_{i=1}^m1\{x_j^{(i)}=1\wedge y^{(i)}=0\}}{\sum_{i=1}^m1\{y^{(i)}=0\}}\\
	      \phi_{j|y=1} &amp;=&amp; \frac{\sum_{i=1}^m1\{x_j^{(i)}=1\wedge y^{(i)}=1\}}{\sum_{i=1}^m1\{y^{(i)}=1\}}\\
	      \phi_y &amp;=&amp; \frac{\sum_{i=1}^m1\{y^{(i)}=1\}}{m}.
	    \end{eqnarray*}
      \]
</p><p><ul style="list-style-type: none";><li><p><strong>Ans.</strong></p></li></ul></p>
<p style="text-align:center">
\[
    \newcommand{\pd}{\partial}
    \begin{eqnarray*}
	    \frac{\pd\ell(\varphi)}{\pd\phi_{j|y=0}}&amp;=&amp;\frac{\pd}{\pd\phi_{j|y=0}}\sum_{i=1}^m\sum_{k=1}^n{x_k^{(i)}\log{\phi_{k|y^{(i)}}}+(1-x_k^{(i)})\log{(1-\phi_{k|y^{(i)}})}}\\
	      &amp;=&amp;\sum_{i=1}^m1\{y^{(i)}=0\}\frac{\pd}{\pd\phi_{j|y=0}}\sum_{k=1}^n{x_k^{(i)}\log{\phi_{k|y=0}}+(1-x_k^{(i)})\log{(1-\phi_{k|y=0})}}\\
	      &amp;=&amp;\sum_{i=1}^m1\{y^{(i)}=0\}\left(\frac{x_j^{(i)}}{\phi_{j|y=0}}-\frac{1-x_j^{(i)}}{1-\phi_{j|y=0}}\right)\\
	      &amp;=&amp;\sum_{i=1}^m1\{y^{(i)}=0\}\left(\frac{x_j^{(i)}-\phi_{j|y=0}}{\phi_{j|y=0}(1-\phi_{j|y=0})}\right)
	  \end{eqnarray*}
    \]
</p><p><ul style="list-style-type: none";><li><p>
Equating the result to \(0\) and solving for \(\phi_{j|y=0}\):</p></li></ul></p>
<p style="text-align:center">
\[
    \begin{eqnarray*}
	    \phi_{j|y=0}&amp;=&amp;\frac{\sum_{i=1}^m{1\{y^{(i)}=0\}}x_j^{(i)}}{\sum_{i=1}^m{1\{y^{(i)}=0\}}}\\
			&amp;=&amp;\frac{\sum_{i=1}^m1\{x_j^{(i)}=1\wedge y^{(i)}=0\}}{\sum_{i=1}^m1\{y^{(i)}=0\}}
	  \end{eqnarray*}
    \]
</p><p><ul style="list-style-type: none";><li><p>
The same procedure can be applied to solve for \(\phi_{j|y=1}\):</p></li></ul></p>
<p style="text-align:center">
\[
    \begin{eqnarray*}
	    \phi_{j|y=1}&amp;=&amp;\frac{\sum_{i=1}^m{1\{y^{(i)}=1\}}x_j^{(i)}}{\sum_{i=1}^m{1\{y^{(i)}=1\}}}\\
			&amp;=&amp;\frac{\sum_{i=1}^m1\{x_j^{(i)}=1\wedge y^{(i)}=1\}}{\sum_{i=1}^m1\{y^{(i)}=1\}}
	  \end{eqnarray*}
    \]
</p><p><ul style="list-style-type: none";><li><p>
Then to solve for \(\phi_y\):</p></li></ul></p>
<p style="text-align:center">
\[
    \begin{eqnarray*}
	    \frac{\pd\ell(\varphi)}{\pd\phi_y}&amp;=&amp;\frac{\pd}{\pd\phi_y}{\sum_{i=1}^m{y^{(i)}\log{\phi_y}+(1-y^{(i)})\log{(1-\phi_y)}}}\\
	      &amp;=&amp;{\sum_{i=1}^m{\frac{1\{y^{(i)}=1\}}{\phi_y}-\frac{1\{y^{(i)}=0\}}{1-\phi_y}}}\\
	      &amp;=&amp;{\sum_{i=1}^m{\frac{1\{y^{(i)}=1\}(1-\phi_y)-1\{y^{(i)}=0\}\phi_y}{\phi_y(1-\phi_y)}}}\\
	  \end{eqnarray*}
    \]
</p><p><ul style="list-style-type: none";><li><p>
Again, equating to \(0\) and then solving for \(\phi_y\) we get</p></li></ul></p>
<p style="text-align:center">
\[
    \begin{eqnarray*}
	    \phi_y &amp;=&amp; \frac{\sum_{i=1}^m1\{y^{(i)}=1\}}{\sum_{i=1}^m1\{y^{(i)}=1\}+1\{y^{(i)}=0\}}\\
		   &amp;=&amp; \frac{\sum_{i=1}^m1\{y^{(i)}=1\}}{m}
	  \end{eqnarray*}
    \]
</p><ul>
<li><p>Consider making a prediction on some new data point \(x\) using the most likely class estimate generated by the naive Bayes algorithm.
Show that the hypothesis returned by naive Bayes is a linear classifier&#8201;&mdash;&#8201;i.e., if \(p(y=0|x)\) and \(p(y=1|x)\) are the class
probabilities returned by naive Bayes, show that there exists some \(\theta\in\mathbb{R}^{n+1}\) such that</p>
</li>
</ul>
<p style="text-align:center">
\[
	      p(y=1|x)\geq p(y=0|x)\textrm{ if and only if }\theta^T\left[\begin{array}{c}1\\x\end{array}\right]\geq 0
	   \]
</p><p><ul style="list-style-type: none";><li><p>
(Assume \(\theta_0\) is an intercept term.)</p></li></ul></p>
<p><ul style="list-style-type: none";><li><p><strong>Ans.</strong></p></li></ul></p>
<p style="text-align:center">
\[
        \begin{eqnarray*}
		p(y=1|x) &amp;\geq&amp; p(y=0|x)\\
		\frac{p(y=1|x)}{p(y=0|x)} &amp;\geq&amp; 1\\
		\frac{p(x|y=1)p(y=1)}{p(x|y=0)p(y=0)} &amp;\geq&amp; 1\\
		\frac{\phi_y\prod_{j=1}^n(\phi_{j|y=1})^{x_j}(1-\phi_{j|y=1})^{1-x_j}}{(1-\phi_y)\prod_{j=1}^n(\phi_{j|y=0})^{x_j}(1-\phi_{j|y=0})^{1-x_j}}  &amp;\geq&amp; 1\\
		\log\left(\frac{\phi_y}{1-\phi_y}\right)+\sum_{j=1}^n\left[x_j\log\left(\frac{\phi_{j|y=1}}{\phi_{j|y=0}}\right)+(1-x_j)\log\left(\frac{1-\phi_{j|y=1}}{1-\phi_{j|y=0}}\right)\right] &amp;\geq&amp; 0\\
		\underbrace{\log\left(\frac{\phi_y}{1-\phi_y}\right)+\sum_{j=1}^n\log\left(\frac{1-\phi_{j|y=1}}{1-\phi_{j|y=0}}\right)}_{\theta_0}
		  +\sum_{j=1}^nx_j\underbrace{\log\frac{\phi_{j|y=1}(1-\phi_{j|y=0})}{\phi_{j|y=0}(1-\phi_{j|y=1})}}_{\theta_j} &amp;\geq&amp; 0
	      \end{eqnarray*}
        \]
</p><p><ul style="list-style-type: none";><li><p>
And \(\theta^T\left[\begin{array}{c}1\\x\end{array}\right] \geq 0\) follows from the last inequality.</p></li></ul></p>
<p><a href="mlpb_2020-10-24_5.html" target=&ldquo;blank&rdquo;>Next problem</a></p>
</div> <!-- <div id="layout-content"> -->
<div id="footer-container">
<div id="footer">
<div id="footer-text">
Last edited  on April 14<sup>th</sup> 2021  05:45PM (Time Zone: JST). </br>
Powered by <a href="https://github.com/szl2/jemdoc-new-design" target="blank">jemdoc + new design</a>.
</div> <!-- <div id="footer-text"> -->
</div> <!-- <div id="footer"> -->
</div> <!-- <div id="footer-container"> -->
</div> <!-- <div id="layout-content-container"> -->
</div> <!--- <div id="layout"> --->
</div> <!--- <div id="main-container"> --->
<script>
function openNav() {
    if (window.innerWidth <= 1200) {
        document.getElementById("layout-menu").style.width = "280px";
        document.getElementById("layout-content-container").style.marginLeft = "280.8px";
        document.getElementById("layout-content-container").style.position = "fixed";
    }
}
function closeNav() {
    if (window.innerWidth <= 1200) {
        document.getElementById("layout-menu").style.width = "0";
        document.getElementById("layout-content-container").style.position = "static";
        document.getElementById("layout-content-container").style.marginLeft = "0px";
        setInterval(
            function(){ location.reload() },
            500
        );
    }
}
</script>
</body>
</html>
